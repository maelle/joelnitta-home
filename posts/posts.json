[
  {
    "path": "posts/2022-01-24_friday-ferns-2021/",
    "title": "#fernfriday 2021",
    "description": "In which I answer the burning question on everybody's minds: what was\nthe most popular fern of 2021??",
    "author": [
      {
        "name": "Joel Nitta",
        "url": "https://joelnitta.com"
      }
    ],
    "date": "2022-01-24",
    "categories": [],
    "contents": "\n\nContents\ntl;dr\nGetting tweets with snscrape\nInstallation\nScraping\nLoading tweets into R\n\nExtracting scientific names with gntagger\nTagging names\nLoading gntagger output into R\n\nData analysis (finally!)\n\n\n\n\n\n\ntl;dr\nI show how to analyze #fernfriday tweets from 2021, including\nScraping tweets with snscraper\nExtracting species names with gntagger\nLoading JSON data into R with jsonlite\n\n\n\nFigure 1: Image by jewelie108 on pixabay.\n\n\n\nOne of the saving graces of twitter is the existence of things like #fernfriday and other hashtags of people‚Äôs favorite organisms.\nRecently, @HannahOish posted this brief analysis of the #FishADay hashtag:\n\n\n\nUpon seeing it, I immediately knew what I had to do next: find out the most popular fern species of #fernfriday in 2021!\nFirst let‚Äôs load some packages.\n\n\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(janitor)\nlibrary(fuzzyjoin)\n\n\n\nGetting tweets with snscrape\nThere are at least two R packages to get twitter data in R, rtweet and tweetR\nThere is just one problem: they rely on the official twitter API, which is limited to tweets in the past week to 10 days or so, unless you want to shell out $$$ for a paid API account‚Ä¶ which I don‚Äôt.\nSo I will get around this pesky problem by using the snscrape twitter scraper written in python 1.\nInstallation\nsnscrape can be installed using pip.\nMy preferred method for handling python is to use the conda environment manager. We can use pip within conda to install snscrape by writing an environment.yml file like this (thanks to @hansheng0512):\nname: tweets-scraping\ndependencies:\n- python=3.8\n- pip=21.1.3\n- pip:\n- git+https://github.com/JustAnotherArchivist/snscrape.git\nOnce that is in your working directory, run this to create the environment:\nconda env create -f environment.yml\nNow snscrape should be installed and ready to use!\nScraping\nSince snscrape is a command-line interface (CLI), the normal way to use it would be to type commands from the command line. But this is an R blog, so let‚Äôs run it from R using the system() command 2 3.\n\n\n# Set some variables, in case we want to modify the search later\nDATE_START <- \"2021-01-01\"\nDATE_END <- \"2021-12-31\"\nHASHTAG <- \"fernfriday\"\nJSON_FILENAME = \"ff-2021-tweets\"\n\n# Compose the command for snscrape to retrieve tweets\ncommand <- glue::glue('snscrape --jsonl --since {DATE_START} twitter-hashtag \"{HASHTAG} until:{DATE_END}\" > {JSON_FILENAME}.json')\n\n# Run the command inside the conda env\nsystem(\n  glue::glue(\n    \"source ~/miniconda3/etc/profile.d/conda.sh ; \n       conda activate tweets-scraping ; \n       {command} ; \n       conda deactivate\")\n)\n\n\n\nNow the tweets have been downloaded in JSON format to ff-2021-tweets.json.\nLoading tweets into R\nWe can load the tweets into R with the jsonlite package.\n\n\nff_tweets_raw <- jsonlite::stream_in(file(\"ff-2021-tweets.json\"))\n\n\n\n\n\n Found 500 records...\n Found 1000 records...\n Found 1345 records...\n Imported 1345 records. Simplifying...\n\nLet‚Äôs take a peek at the data.\n\n\nglimpse(ff_tweets_raw)\n\n\nRows: 1,345\nColumns: 28\n$ `_type`          <chr> \"snscrape.modules.twitter.Tweet\", \"snscrape‚Ä¶\n$ url              <chr> \"https://twitter.com/bmoandkmo/status/14766‚Ä¶\n$ date             <chr> \"2021-12-30T23:31:27+00:00\", \"2021-12-30T23‚Ä¶\n$ content          <chr> \"the last #FernFriday of this year\\n\\nAmazi‚Ä¶\n$ renderedContent  <chr> \"the last #FernFriday of this year\\n\\nAmazi‚Ä¶\n$ id               <dbl> 1.476697e+18, 1.476690e+18, 1.476681e+18, 1‚Ä¶\n$ user             <df[,23]> <data.frame[23 x 23]>\n$ replyCount       <int> 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ‚Ä¶\n$ retweetCount     <int> 1, 0, 1, 0, 0, 1, 0, 0, 5, 1, 0, 0, 3, 1, 1‚Ä¶\n$ likeCount        <int> 6, 3, 22, 4, 6, 7, 4, 35, 21, 27, 14, 15, 3‚Ä¶\n$ quoteCount       <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ conversationId   <dbl> 1.476697e+18, 1.476690e+18, 1.476681e+18, 1‚Ä¶\n$ lang             <chr> \"en\", \"und\", \"und\", \"und\", \"und\", \"und\", \"u‚Ä¶\n$ source           <chr> \"<a href=\\\"https://mobile.twitter.com\\\" rel‚Ä¶\n$ sourceUrl        <chr> \"https://mobile.twitter.com\", \"https://mobi‚Ä¶\n$ sourceLabel      <chr> \"Twitter Web App\", \"Twitter Web App\", \"Twit‚Ä¶\n$ outlinks         <list> <NULL>, <NULL>, <NULL>, <NULL>, <NULL>, <NU‚Ä¶\n$ tcooutlinks      <list> <NULL>, <NULL>, <NULL>, <NULL>, <NULL>, <N‚Ä¶\n$ media            <list> [<data.frame[2 x 3]>], [<data.frame[1 x 3]‚Ä¶\n$ retweetedTweet   <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n$ quotedTweet      <df[,28]> <data.frame[23 x 28]>\n$ inReplyToTweetId <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ inReplyToUser    <df[,23]> <data.frame[23 x 23]>\n$ mentionedUsers   <list> <NULL>, <NULL>, <NULL>, <NULL>, <NULL>,‚Ä¶\n$ coordinates      <df[,3]> <data.frame[23 x 3]>\n$ place            <df[,6]> <data.frame[23 x 6]>\n$ hashtags         <list> \"FernFriday\", \"FernFriday\", <\"FernFriday\", ‚Ä¶\n$ cashtags         <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n\nThere‚Äôs a lot of stuff in there! Let‚Äôs clean up the data a bit and just keep the useful bits.\n\n\nff_tweets <-\n  ff_tweets_raw %>%\n    as_tibble() %>%\n    select(\n      date, url, user, id,\n      content = renderedContent,\n      lang, coordinates, place,\n      contains(\"count\")\n    ) %>%\n  # Extract data from nested dataframes \n  mutate(\n    user = select(user, username, followersCount),\n    place = select(place, country),\n    coordinates = select(coordinates, longitude, latitude)\n    ) %>%\n  unnest(c(user, place, coordinates)) %>%\n  clean_names()\n\nff_tweets\n\n\n# A tibble: 1,345 √ó 14\n   date    url       username followers_count      id content    lang \n   <chr>   <chr>     <chr>              <int>   <dbl> <chr>      <chr>\n 1 2021-1‚Ä¶ https://‚Ä¶ bmoandk‚Ä¶              79 1.48e18 \"the last‚Ä¶ en   \n 2 2021-1‚Ä¶ https://‚Ä¶ ALULUAL‚Ä¶             203 1.48e18 \"#FernFri‚Ä¶ und  \n 3 2021-1‚Ä¶ https://‚Ä¶ vatanek‚Ä¶             740 1.48e18 \"#FernFri‚Ä¶ und  \n 4 2021-1‚Ä¶ https://‚Ä¶ ALULUAL‚Ä¶             203 1.48e18 \"#FernFri‚Ä¶ und  \n 5 2021-1‚Ä¶ https://‚Ä¶ ALULUAL‚Ä¶             203 1.48e18 \"#FernFri‚Ä¶ und  \n 6 2021-1‚Ä¶ https://‚Ä¶ ALULUAL‚Ä¶             203 1.48e18 \"#FernFri‚Ä¶ und  \n 7 2021-1‚Ä¶ https://‚Ä¶ ALULUAL‚Ä¶             203 1.48e18 \"#FernFri‚Ä¶ und  \n 8 2021-1‚Ä¶ https://‚Ä¶ duckinw‚Ä¶            1307 1.47e18 \"Wishing ‚Ä¶ en   \n 9 2021-1‚Ä¶ https://‚Ä¶ helecho‚Ä¶             208 1.47e18 \"¬°Felicid‚Ä¶ pt   \n10 2021-1‚Ä¶ https://‚Ä¶ KOKESHI‚Ä¶            1506 1.47e18 \"ÔºÉFernFr‚Ä¶ en   \n# ‚Ä¶ with 1,335 more rows, and 7 more variables: longitude <dbl>,\n#   latitude <dbl>, country <chr>, reply_count <int>,\n#   retweet_count <int>, like_count <int>, quote_count <int>\n\nThat‚Äôs better.\nExtracting scientific names with gntagger\nSo getting our twitter data wasn‚Äôt that hard, now just to find all the fern species‚Ä¶\nUh-oh. This is easier said than done. Twitter doesn‚Äôt have a JSON field for fern species!\nLuckily this post does have some biological content. For this task I will use gntagger. gntagger is a fantastic little CLI written in GO that handles exactly this sort of situation: it automatically detects species names in raw text 4.\nInstallation is quite simple as described in the gntagger docs, so I won‚Äôt go into that any more here.\nTagging names\ngntagger requires a plain-text file as input, so let‚Äôs write out the tweets to a plain text file with one line per tweet:\n\n\n\n\n\nff_tweets %>%\n  # Replace line breaks with spaces so we get one tweet per line\n  mutate(content = str_replace_all(content, \"\\n\", \" \")) %>%\n  pull(content) %>%\n  write_lines(\"ff-2021-tweet-content.txt\")\n\n\n\nNow, open the text file with gntagger 5:\ngntagger ff-2021-tweet-content.txt\nYou should see a screen that looks like this:\n\ngntagger takes a ‚Äúandroid‚Äù approach to tagging species names. It first uses its own algorithm to automatically identify all possible species names, then it has a simple user interface so the user can confirm or reject each candidate name rapidly. According to the gntagger docs, you should be able to get through about ‚Äú4000 names spread over 600 pages in about 2 hours‚Äù. It took me around 10‚Äì15 min. for this dataset, which included 664 candidate names to process.\nIt saves the intermediate output to a folder named after the input file, in this case ff-2021-tweet-content.txt_gntagger.\nLoading gntagger output into R\nI recommend trying out gntagger to see how it works‚Äîit definitely looks great for parsing names from old literature!\nBut you don‚Äôt have to go through all of the names yourself. In fact, gntagger‚Äôs initial ‚Äúguesses‚Äù are pretty darn good, and can be used as-is for a rough analysis. Or, you can download the cleaned up version from FIXME: ADD LINK.\nLet‚Äôs read in the gntagger results, which are again JSON:\n\n\nff_taxa_raw <- jsonlite::fromJSON(\"ff-2021-tweet-content.txt_gntagger/names.json\")\n\nff_taxa <- ff_taxa_raw[[\"names\"]] %>% \n  as_tibble() %>%\n    # I'll call the name output by gntagger \"taxon\", since\n  # not all of them are species\n  select(type, annotation, taxon = name, start, end)\n\nff_taxa\n\n\n\n\n# A tibble: 630 √ó 5\n   type             annotation taxon                  start   end\n   <chr>            <chr>      <chr>                  <int> <int>\n 1 Binomial         Accepted   Elaphoglossum crinitum   430   452\n 2 Binomial         Accepted   Aglaomorpha heraclea    1230  1250\n 3 Binomial         Accepted   Arachniodes standishii  1325  1347\n 4 Uninomial        Genus      Platycerium             1398  1409\n 5 PossibleBinomial Accepted   P. alcicorne            1562  1574\n 6 Binomial         Accepted   Angiopteris evecta      2457  2475\n 7 Binomial         Accepted   Dryopteris shibipedis   2508  2529\n 8 Uninomial        Genus      Pyrrosia                2914  2923\n 9 Binomial         Accepted   Doodia australis        3161  3178\n10 Binomial         Accepted   Lastreopsis hispida     3189  3209\n# ‚Ä¶ with 620 more rows\n\nA bit about the column names we are reading in from gntagger:\ntype is something automatically defined by gntagger. I‚Äôm not quite sure what it means‚Ä¶ but I‚Äôm guessing ‚ÄúBinomial‚Äù indicates that the name is a species.\nannotation is the annotation I selected during the tagging process. It includes values ‚ÄúAccepted‚Äù, ‚ÄúGenus‚Äù, ‚ÄúNotName‚Äù, ‚ÄúSpecies‚Äù, and ‚ÄúUninomial‚Äù. It wasn‚Äôt totally clear to me how to apply these during the tagging part. I mostly just hit the forward arrow for names that looked like species, so those are annotated as ‚ÄúAccepted‚Äù. Towards the end I noticed I could select ‚ÄúSpecies‚Äù, so a few are annotated as ‚ÄúSpecies‚Äù instead of ‚ÄúAccepted‚Äù 6.\nstart and end indicate the character position in the raw text matching the start and end of the name.\nThe next step is to use the start and end fields to match species names to tweets. For that, we‚Äôll need a tibble with the start and end position of each tweet.\n\n\nff_tweets_start_end <-\n  ff_tweets %>%\n  mutate(\n    # Replace line breaks with spaces so we get one tweet per line\n    content = str_replace_all(content, \"\\n\", \" \"),\n    # Count number of characters per tweet\n    num_char = nchar(content)) %>%\n  select(id, num_char) %>%\n  # Calculate start and end of each tweet in characters\n  mutate(\n    end = cumsum(num_char),\n    start = end - num_char + 1,\n    start = as.integer(start)) %>%\n  select(id, start, end)\n\nff_tweets_start_end\n\n\n# A tibble: 1,345 √ó 3\n        id start   end\n     <dbl> <int> <int>\n 1 1.48e18     1    96\n 2 1.48e18    97   131\n 3 1.48e18   132   188\n 4 1.48e18   189   223\n 5 1.48e18   224   258\n 6 1.48e18   259   293\n 7 1.48e18   294   328\n 8 1.47e18   329   552\n 9 1.47e18   553   623\n10 1.47e18   624   658\n# ‚Ä¶ with 1,335 more rows\n\nNow we can use the fuzzyjoin package to join the species names to the tweets by position.\n\n\nff_tweets_taxa <-\nff_tweets %>%\n  left_join(ff_tweets_start_end, by = \"id\") %>%\n  # interval_left_join requires the Bioconductor IRanges package to be installed\n  # and joins by columns \"start\" and \"end\" by default\n  interval_left_join(\n    ff_taxa\n  ) %>%\n  select(-matches(\"start|end\")) %>%\n  # Drop tweets without any fern names mentioned\n  filter(!is.na(taxon))\n\n\n\nLet‚Äôs take a peek at the results.\n\n\nselect(ff_tweets_taxa, content, taxon)\n\n\n# A tibble: 722 √ó 2\n   content                                            taxon           \n   <chr>                                              <chr>           \n 1 \"Wishing all a happy, safe and peacefull #FernFri‚Ä¶ Elaphoglossum c‚Ä¶\n 2 \"Quedan pocos d√≠as del a√±o.\\n‰ªäÂπ¥„ÇÇÊÆã„Åô„Å®„Åì„Çç„ÅÇ„Å®‚Ä¶ Aglaomorpha her‚Ä¶\n 3 \"#FernFriday \\n\\nArachniodes standishii https://t‚Ä¶ Arachniodes sta‚Ä¶\n 4 \"üéÑüéâMerry ChristmaseratiüéÖüéÅ\\n\\nPlatycerium Mase‚Ä¶ Platycerium     \n 5 \"#FernFriday\\nÂ•Ω„Åç\\nP. alcicorne (Madagascar) htt‚Ä¶ P. alcicorne    \n 6 \"#FernFriday üåÉüåø‚ú®\\n\\nAngiopteris evecta https:/‚Ä¶ Angiopteris eve‚Ä¶\n 7 \"„Ç∑„Éì„Ç§„Çø„ÉÅ„Ç∑„ÉÄ Dryopteris shibipedis #Fernfriday‚Ä¶ Angiopteris eve‚Ä¶\n 8 \"„Ç∑„Éì„Ç§„Çø„ÉÅ„Ç∑„ÉÄ Dryopteris shibipedis #Fernfriday‚Ä¶ Dryopteris shib‚Ä¶\n 9 \"‚úåÔ∏è\\n\\n #Fernfriday https://t.co/LdQgp4Wcw0\"        Dryopteris shib‚Ä¶\n10 \"I told my friend that if I ever had a kid, I wou‚Ä¶ Pyrrosia        \n# ‚Ä¶ with 712 more rows\n\nYay! We‚Äôve got fern names mapped to tweets (and user ID, etc). Notice that some tweets now appear duplicated, since our data are have one row per fern name mentioned, and some tweets may include multiple names.\nData analysis (finally!)\nOK, we can finally analyze the data and see who is the most popular fern of 2021!\nLike any good data analysis, let‚Äôs check the distribution of the data first.\n\n\nff_tweets_taxa %>%\n  # Filter to only taxa that look like species names\n  filter(str_detect(type, \"Binomial\")) %>%\n  count(taxon) %>%\n  ggplot(aes(x = n, fill = n == 1)) + \n  geom_histogram(bins = 20) +\n  labs(fill = \"Singleton\") +\n  scale_fill_viridis_d() +\n  labs(x = \"n species\", y = \"n tweets\")\n\n\n\n\nFigure 2: Histogram of species occurrences in tweets\n\n\n\nSo the vast majority of species are only tweeted about once (‚Äúsingletons‚Äù in the plot), then there‚Äôs a longer tail of a much smaller number of species that receive multiple tweets. Sort of like ecology‚Äîcommon species are rare, and rare species are common. Neat.\nNow that we have a feel for how the data are distributed, let‚Äôs see who are the rare species with multiple tweets. It‚Äôs possible that some users are tweeting the same species multiple times, and that seems sort of unfair right? So we will just consider one tweet per species per user, and look at the top 10.\n\n\nff_tweets_taxa %>% \n  # Filter to only taxa that look like species names\n  filter(str_detect(type, \"Binomial\")) %>%\n  # Only allow one \"vote\" per species per user\n  select(taxon, username) %>% \n  unique() %>% \n  # Just look at the top 10\n  # fct_lump_n() lumps the remainder into \"Other\"\n  mutate(taxon = fct_lump_n(taxon, n = 10)) %>%\n  # Exclude \"Other\"\n  filter(taxon != \"Other\") %>%\n  count(taxon) %>%\n  mutate(taxon = fct_reorder(taxon, n)) %>%\n  ggplot(aes(x = n, y = taxon)) + \n  geom_col() +\n  labs(x = \"n users tweeting\") +\n  theme(axis.title.y = element_blank())\n\n\n\n\nFigure 3: Top 10 species by number of users tweeting\n\n\n\nCool! Those are some solid picks. And the top 10 species here all include > 3 three unique users tweeting about them.\nWhat about the most popular genera?\n\n\nff_tweets_taxa %>% \n  # Split species names into genus and specific epithet\n  separate(\n    taxon, into = c(\"genus\", \"specific_epithet\"), sep = \" \",\n    fill = \"right\", extra = \"drop\") %>%\n  # Exclude names that I annotated as \"Uninomial\" (family names)\n  filter(annotation != \"Uninomial\") %>%\n  # Exclude abbreviated genera (\"P.\", etc)\n  mutate(n_genus_char = nchar(genus)) %>%\n  filter(n_genus_char > 3) %>%\n  # Only allow one \"vote\" per species per user\n  select(genus, username) %>% \n  unique() %>% \n  # Just look at the top 10\n  # fct_lump_n() lumps the remainder into \"Other\"\n  mutate(genus = fct_lump_n(genus, n = 10)) %>%\n  # Exclude \"Other\"\n  filter(genus != \"Other\") %>%\n  count(genus) %>%\n  mutate(genus = fct_reorder(genus, n)) %>%\n  ggplot(aes(x = n, y = genus)) + \n  geom_col() +\n  labs(x = \"n users tweeting\") +\n  theme(axis.title.y = element_blank())\n\n\n\n\nFigure 4: Top 10 genera by number of users tweeting\n\n\n\nThe trends change a bit: although no single Asplenium species gets lots of tweets, the genus as whole does. Cool!\nThere is obviously a lot that can be done with twitter data, and text data in general. I won‚Äôt be going there. But I just want to look at one more thing‚Ä¶\nWho are the most prolific #fernfriday tweeters?\n\n\nff_user_count <-\n  ff_tweets %>%\n  count(username) %>%\n  mutate(username = fct_reorder(username, n))\n\nggplot(ff_user_count, aes(x = n, y = username)) +\n  geom_col(aes(fill = n == 1)) +\n  # Only label the top 10\n  geom_label_repel(\n    data = slice_max(ff_user_count, order_by = n, n = 10), \n    aes(label = username),\n    force = 3,\n    box.padding = 0.5,\n    min.segment.length = 0, \n    max.overlaps = Inf,\n    direction = \"y\") +\n  # ... plus me\n  geom_label_repel(\n    data = filter(ff_user_count, username == \"joel_nitta\"),\n    fill = \"light blue\",\n    min.segment.length = 0,\n    aes(label = username)) +\n  scale_fill_viridis_d() +\n  scale_x_continuous(expand = c(0,0)) +\n  labs(x = \"n tweets\", fill = \"Singleton\") +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    legend.position = \"bottom\")\n\n\n\n\nFigure 5: Number of #fernfriday tweets per user\n\n\n\nWow, some people are really on top of this hashtag! I recognize some of these‚Ä¶ @ALULUALULU_M takes really nice photos, and @ja_pelosi has been posting informative threads about one fern family per week at #51WeeksOfPteridophytes.\nAgain we see a similar pattern to species: there are a small number of very regular #fernfriday tweeters, then a long tail of those who use the hashtag only occasionally‚Ä¶ or just once (ahem)!\nWell I might not have the most #fernfriday tweets, but I think I‚Äôm definitely in the running for best #fernfriday blogpost :)\n\n\nLast updated\n2022-01-27 18:18:11 JST\nDetails\nsource code, R environment\n\n\n\n\n\n\nThis is probably not within twitter‚Äôs user guidelines, so use at your own discretion‚Ü©Ô∏é\nA good way to limit the number of hits when you are testing code is with the --max-results option, e.g., --max-results 10‚Ü©Ô∏é\nAgain, inspired by @hansheng0512‚Äôs code‚Ü©Ô∏é\nOK it was probably designed more with old biodiversity literature in mind and not twitter, but it works fine either way!‚Ü©Ô∏é\nYou may need to execute this with ./gntagger if you didn‚Äôt make gntagger executable everywhere by putting it on your PATH‚Ü©Ô∏é\nI actually think using a single variable for both taxonomic status (‚ÄúAccepted‚Äù) and rank (‚ÄúSpecies‚Äù, ‚ÄúGenus‚Äù, etc) is rather confusing, and have filed an issue about this‚Ü©Ô∏é\n",
    "preview": "posts/2022-01-24_friday-ferns-2021/img/new-zealand-fantail-3729278_960_720.jpg",
    "last_modified": "2022-03-07T08:13:41+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-13_r-bioinfo-pt2/",
    "title": "Managing bioinformatics pipelines with R (Part 2)",
    "description": "An update on managing bioinformatics pipelines with R, mainly to highlight a nifty new function that allows you to (more) easily run Docker containers.",
    "author": [
      {
        "name": "Joel Nitta",
        "url": "https://joelnitta.com"
      }
    ],
    "date": "2022-01-13",
    "categories": [],
    "contents": "\n\n\n\n\n\n\nAs I previous blogged about, R is a great tool for managing bioinformatics workflows. At least, I think it has that potential, but it isn‚Äôt used for this purpose very widely yet. I recently figured out a trick that makes it easier to use, which I describe below.\nIf you haven‚Äôt read my first blogpost on this topic, I recommend reading that first to get up to speed.\nDocker and babelwhale\nAs mentioned previously mentioned, we can call Docker from R using the babelwhale package, in particular the run() function.\nbabelwhale::run() can be thought of as a thin wrapper around docker run. Here is a plain text view of how they compare:\ndocker run [OPTIONS] IMAGE [COMMAND] [ARG...]\n              |       |         |        |  \n              |       |         |        |                \n              \\-------|---------|--------|--------\\  \n                      |         |        |        |\n                      |         |        |        |\nbabelwhale::run(container_id, command, args, volumes)\nVolume headaches\nUntil recently, my typical workflow has been to write a wrapper function for each docker command that I can execute from R using babelwhale::run(). However, I quickly noticed that the process of specifying volumes for files was tedious and repetitive. Each time I had to think about the paths to use for the files inside the container, and how they should relate to the paths outside the container. Wouldn‚Äôt it be great if we could abstract this away and just automatically mount files?\nAs it turns out, we can! I realized that docker is pretty open-minded when it comes to volume mounts. For example, it‚Äôs no problem to have multiple volumes mounted to one container, and the paths inside the container can be arbitrary as long as they don‚Äôt clobber any existing folders‚ÄîDocker will create them for us as needed.\n\n\nLast updated\n2022-01-13 18:26:43 JST\nDetails\nsource code, R environment\n\n\n\n\n\n\n\n\n",
    "preview": "posts/2022-01-13_r-bioinfo-pt2/img/bart-van-meele-ns4bIZ6XeGY-unsplash.jpg",
    "last_modified": "2022-03-07T08:13:41+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-24_using-giscus/",
    "title": "Enable giscus in Distill",
    "description": "How to use the giscus commenting system on a Distill blog",
    "author": [
      {
        "name": "Joel Nitta",
        "url": "https://joelnitta.com"
      }
    ],
    "date": "2021-11-24",
    "categories": [],
    "contents": "\n\n\n\n\n\nTL;DR\ngiscus is a free, open-source commenting system for blogs that uses the GitHub API\ngiscus uses GitHub Discussions (not Issues) to store data\nI show how to enable giscus on a Distill blog\n\n\n\nFigure 1: Image by Adam Solomon on unsplash.\n\n\n\nLike many R-bloggers these days, I have made some changes: I switched from blogdown to Distill 1, and from disqus to utterances 2. Several things about utterances appealed to me: free, open-source, no data tracking. But when I started using it, I immediately was turned off by the dual use of GitHub issues as a way to store comments. It just felt odd to have an issue that wasn‚Äôt an issue!\nFortunately, I‚Äôm not the only one to feel this way, and @laymonage actually did something about it: there is now a very similar app to utterances, called giscus. It offers almost the same functionality, but it uses GitHub Discussions as the place to store comments instead of Issues. This makes much more sense to me.\nThere are several blogposts 3 on how to enable utterances on Distill, but none that I‚Äôve found so far on giscus. So, here goes!\nEnable Discussions on your blog repo. Optionally, if you want to use a non-default Discussions category for storing giscus content, add a new category. I did this and called it ‚ÄúComments‚Äù. As recommended by giscus, it‚Äôs a good idea to set the discussion format to ‚ÄúAnnouncement‚Äù so that non-authorized users can‚Äôt add content via the Discussions interface (only the giscus widget on your blog).\nInstall the giscus GitHub app and configure it to have access to your blog‚Äôs repo.\nGo to the giscus app interface, scroll down to ‚Äúconfiguration‚Äù and fill in the details for your blog. Once you‚Äôve done so, further down you should see an HTML code block under ‚ÄúEnable giscus‚Äù populated with your information.\n\n\n\nFigure 2: giscus configuration menu.\n\n\n\n\n\n\nFigure 3: giscus HTML block. Once you fill in the fields in the configuration menu, the parts starting with [ENTER ...] will get automatically populated.\n\n\n\nAs described in Miles McBain‚Äôs blogpost, unfortunately in Distill, you can‚Äôt just paste the HTML directly into an Rmd file. It won‚Äôt show up. But the same work-around that he describes for utterances also happily works for giscus! Read on‚Ä¶\nAdd an .html file (I‚Äôve called mine giscus.html) to the root of your blog repo that looks like this (and is based off of Miles‚Äô HTML):\n\n<script>\n   document.addEventListener(\"DOMContentLoaded\", function () {\n     if (!/posts/.test(location.pathname)) {\n       return;\n     }\n\n     var script = document.createElement(\"script\");\n     script.src = \"https://giscus.app/client.js\";\n     script.setAttribute(\"data-repo\", \"[ENTER REPO HERE]\");\n     script.setAttribute(\"data-repo-id\", \"[ENTER REPO ID HERE]\");\n     script.setAttribute(\"data-category\", \"[ENTER CATEGORY NAME HERE]\");\n     script.setAttribute(\"data-category-id\", \"[ENTER CATEGORY ID HERE]\");\n     script.setAttribute(\"data-mapping\", \"pathname\");\n     script.setAttribute(\"data-reactions-enabled\", \"0\");\n     script.setAttribute(\"data-emit-metadata\", \"0\");\n     script.setAttribute(\"data-theme\", \"light\");\n     script.setAttribute(\"data-lang\", \"en\");\n\n     /* wait for article to load, append script to article element */\n     var observer = new MutationObserver(function (mutations, observer) {\n       var article = document.querySelector(\"d-article\");\n       if (article) {\n         observer.disconnect();\n         /* HACK: article scroll */\n         article.setAttribute(\"style\", \"overflow-y: hidden\");\n         article.appendChild(script);\n       }\n     });\n\n     observer.observe(document.body, { childList: true });\n   });\n <\/script>\n\nIf you compare the above code with the HTML block in the giscus app (Fig. 3), you should be able to see how the script.setAttribute lines above map to the key-value pairs in the HTML block in the giscus app. All we have to do is copy the contents of the HTML block over to this giscus.html file. You can see what my giscus.html file looks like here.\nModify _site.yml so that the giscus.html file gets loaded on every Distill article page 4:\noutput: \n  distill::distill_article:\n    includes:\n      in_header: giscus.html\nThat‚Äôs it! Or it should be anyways. I recommend trying a test comment to make sure everything is working (nobody will tell you otherwise‚Ä¶)\n\n\nLast updated\n2021-11-29 10:35:23 JST\nDetails\nsource code, R environment\n\n\n\n\n\n\nblogdown and Distill are R packages for making websites. In a nutshell, Distill is much simpler to use than blogdown, at the cost of some design flexibility. For more about making the switch, you can get caught up with posts from Thomas Mock, Frie Preu, Lisa Lendway, and Andreas Handel.‚Ü©Ô∏é\ndisqus and utterances are tools that let users comment on blog posts. Recently many R-bloggers have been moving away from disqus because it has a habit of tracking user‚Äôs data and causing page bloat. More recently, when I checked on my disqus account (in the process of migrating away!), it had a option to ‚Äúopt-out‚Äù of data tracking, but that means data-tracking is on by default.‚Ü©Ô∏é\nFor example, Vebash Naidoo‚Äôs tutorial and Michael McCarthy‚Äôs post describing how to control the location of the comments section.‚Ü©Ô∏é\nThe _site.yml file is longer than this, but I‚Äôm just showing the relevant code to add. You can see my _site.yml file here.‚Ü©Ô∏é\n",
    "preview": "posts/2021-11-24_using-giscus/img/adam-solomon-WHUDOzd5IYU-unsplash.jpg",
    "last_modified": "2022-03-07T08:13:41+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-16_r-bioinfo-flow/",
    "title": "Managing bioinformatics pipelines with R",
    "description": "How to combine Conda, Docker, and R to run modular, reproducible bioinformatics pipelines",
    "author": [
      {
        "name": "Joel Nitta",
        "url": "https://joelnitta.com"
      }
    ],
    "date": "2021-11-16",
    "categories": [],
    "contents": "\n\n\n\n\n\n\n\ntl;dr\nThe targets R package is great for managing bioinformatics workflows\nrenv, Conda, and Docker can be combined so that all steps are modular and reproducible\nDemo available at https://github.com/joelnitta/targets_bioinfo_example\n\n\n\nFigure 1: Image by T K on unsplash.\n\n\n\nBioinformatics projects tend to have a similar pattern: they all start with raw data, then pass the data through various programs until arriving at the final result. These ‚Äúpipelines‚Äù can become very long and complicated, so there are many platforms that automate this process either relying on code (e.g., nextflow, CWL) or graphical interfaces (e.g., galaxy). Python‚Äôs snakemake is also commonly used for this purpose. That got me thinking - can we do this in R?\nWhat I‚Äôm looking for in a pipeline manager\nThese are some qualities that I want to see in a pipeline manager.\nAutomated: I should be able to run one central script that will orchestrate the whole pipeline, rather than manually keeping track of which step depends on which and when each needs to be run.\nEfficient: The pipeline manager should keep track of what is out of date and only re-run those parts, rather than re-run the whole pipeline each time.\nReproducible: Software packages should be isolated and version controlled so that the same input results in the same output on any machine.\nEnter targets\nThe targets R package pretty much fits the bill perfectly for Points 1 and 2. targets completely automates the workflow, so that the user doesn‚Äôt have to manually run steps, and guarantees that the output is up-to-date (if the workflow is designed correctly). Furthermore, it has capabilities for easily looping and running processes in parallel, so it scales quite well to large analyses. I won‚Äôt go into too many details of how to use targets here, since it has an excellent user manual.\ntargets meets Docker\nHowever, targets by itself isn‚Äôt quite enough to meet all of my bioinformatics needs. What about Point 3‚Äîhow can we make targets workflows reproducible?\nMost bioinformatics tools are open-source software packages that have a command-line interface (CLI). Furthermore, these days, most well-established bioinformatics tools have Docker images1 available to run them. Good sources to find Docker images for bioinformatics software are Bioconda or Biocontainers2. Docker frees us from manual installations and dependency hell, as well as vastly improving reproducibility, since all the software versions are fixed within the container.\nSo I will run most of the steps of the pipeline in available Docker containers.\nAvoiding Docker-in-Docker\n\n\n\nFigure 2: Image by Giordano Rossoni on unsplash.\n\n\n\nHowever, I then encounter a problem: what about the environment to run R, targets, and launch the Docker containers? That environment should be version-controlled and reproducible too. Normally my solution to create such an environment is Docker, but it‚Äôs generally a bad idea to try and run docker from within docker3\nThe solution I reached is to use two more environment managers: Conda4 and renv. I use Conda for running R, and renv for managing R packages.\nFirst things first: Set up the project\nI need to explain one thing before continuing: for this example, I‚Äôm following the practice of using a ‚Äúproject‚Äù for the analysis. This simply means all of the files needed for the analysis are put in a single folder (with subfolders as necessary), and that folder is used as the ‚Äúhome base‚Äù for the project. So if I type some command at the command line prompt, it is assumed that the project folder is the current working directory. The two main tools I use to maintain the pipeline, renv and targets, both rely on this concept.\nFrom the command line, that just looks like:\n\nmkdir targets_bioinfo_example\ncd targets_bioinfo_example\n\nNext, let‚Äôs download some files that I will use in the subsequent steps (don‚Äôt worry about what these do yet; I will explain each one below):\n\n\n# environment.yml\ncurl https://raw.githubusercontent.com/joelnitta/targets_bioinfo_example/main/environment.yml > environment.yml\n# renv.lock\ncurl https://raw.githubusercontent.com/joelnitta/targets_bioinfo_example/main/renv.lock > renv.lock\n# _targets.R\ncurl https://raw.githubusercontent.com/joelnitta/joelnitta-home/main/_posts/2021-11-16_r-bioinfo-flow/_targets.R > _targets.R\n\nFrom here on, I assume we are running everything a folder called targets_bioinfo_example containing the files environment.yml, renv.lock, and _targets.R.\nAlso, although I‚Äôve mentioned several pieces of software so far, there are only two required for this workflow: Conda and Docker. Make sure those are both installed before continuing.\nRunning R with Conda\nConda environments can be specified using a yml file, often named environment.yml.\nThis is the environment.yml file for this project:\nname: bioinfo-example-env\nchannels:\n  - conda-forge\n  - bioconda\n  - defaults\ndependencies:\n  - r-renv=0.14.*\nIt‚Äôs quite short: all it does is install renv and its dependencies (which includes R). Here I‚Äôve specified the most recent major version5 of renv, which will come with R v4.1.1.\nWe can recreate the Conda environment from environment.yml (you should have downloaded it above) with:\n\n\nconda env create -f environment.yml\n\n\nCollecting package metadata (repodata.json): ...working... done\nSolving environment: ...working... done\nPreparing transaction: ...working... done\nVerifying transaction: ...working... done\nExecuting transaction: ...working... done\n#\n# To activate this environment, use\n#\n#     $ conda activate bioinfo-example-env\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n\nAs the output says near the bottom, run conda activate bioinfo-example-env to enter this environment, then from there you can use R as usual with R.\nOn my computer, this looks like:\n(base) Joels-iMac:targets_bioinfo_example joelnitta$ conda activate bioinfo-example-env\n(bioinfo-example-env) Joels-iMac:targets_bioinfo_example joelnitta$ R\n\nR version 4.1.1 (2021-08-10) -- \"Kick Things\"\nCopyright (C) 2021 The R Foundation for Statistical Computing\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n> \nNotice the change from (base) to (bioinfo-example-env), indicating that we are now inside the Conda environment.\nNow we have a fixed version of R, with a fixed version of renv.\nMaintain R packages with renv\nThe next step is to use renv to install and track R package versions. renv does this with a ‚Äúlock file‚Äù, which is essentially a specification of every package needed to run the code, its version, and where it comes from.\nThis is what the entry in the renv.lock file for this project for the package Matrix looks like:\n\"Matrix\": {\n      \"Package\": \"Matrix\",\n      \"Version\": \"1.3-4\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Hash\": \"4ed05e9c9726267e4a5872e09c04587c\"\n    }\nAssuming renv.lock is present in the working directory (you should have downloaded it above), we can install all packages needed for this example by running the following in R within the Conda environment:\n\n\nrenv::activate() # Turn on renv\nrenv::restore() # Install packages\n\n\n\nYou should see something like this6:\nThe following package(s) will be updated:\n\n# CRAN ===============================\n- Matrix          [* -> 1.3-4]\n- R6              [* -> 2.5.1]\n- Rcpp            [* -> 1.0.7]\n- RcppArmadillo   [* -> 0.10.6.0.0]\n- RcppParallel    [* -> 5.1.4]\n- assertthat      [* -> 0.2.1]\n- babelwhale      [* -> 1.0.3]\n- callr           [* -> 3.7.0]\n\n...\nIf you look at the contents of the project directory, you will also notice a new folder called renv that contains all of the R packages we just installed.\nPutting it all together\nOK, now we can run R and Docker from a reproducible environment. What is the best way to run Docker from R? There are some functions in base R for running external commands (system(), system2()) as well as the excellent processx package. Here, though I will use the babelwhale package, which provides some nice wrappers to run Docker (or Singularity)7.\nHere is an example _targets.R file using babelwhale to run Docker. This workflow downloads a pair of fasta files, then trims low-quality bases using the fastp program8:\n\n\nlibrary(targets)\nlibrary(tarchetypes)\nlibrary(babelwhale)\n\n# Set babelwhale backend for running containers\n# (here, we are using Docker, not Singularity)\nset_default_config(create_docker_config())\n\n# Define workflow\nlist(\n  # Download example fastq files\n  tar_file(\n    read_1, { \n      download.file(\n        url = \"https://raw.githubusercontent.com/OpenGene/fastp/master/testdata/R1.fq\",\n        destfile = \"R1.fq\")\n      \"R1.fq\"\n    }\n  ),\n  tar_file(\n    read_2, { \n      download.file(\n        url = \"https://raw.githubusercontent.com/OpenGene/fastp/master/testdata/R2.fq\",\n        destfile = \"R2.fq\")\n      \"R2.fq\"\n    }\n  ),\n  # Clean the fastq file with fastp\n  tar_file(\n    fastp_out, {\n      babelwhale::run(\n        # Name of docker image, with tag specifying version\n        \"quay.io/biocontainers/fastp:0.23.1--h79da9fb_0\",\n        # Command to run\n        command = \"fastp\",\n        # Arguments to the command\n        args = c(\n          # fastq input files\n          \"-i\", paste0(\"/wd/\", read_1), \n          \"-I\", paste0(\"/wd/\", read_2), \n          # fastq output files\n          \"-o\", \"/wd/R1_trim.fq\",\n          \"-O\", \"/wd/R2_trim.fq\",\n          # trim report file\n          \"-h\", \"/wd/trim_report.html\"),\n        # Volume mounting specification\n        # this uses getwd(), but here::here() is also a good method\n        volumes = paste0(getwd(), \":/wd/\")\n      )\n      c(\"R1_trim.fq\", \"R2_trim.fq\", \"trim_report.html\")\n    }\n  )\n)\n\n\n\nIn order to run this targets workflow, the above code must be saved as _targets.R in the project root directory (you should have downloaded it above).\nFinally, everything is in place! All we need to do now is run targets::tar_make(), sit back, and enjoy the show:\n\n\ntargets::tar_make()\n\n\n‚Ä¢ start target read_1\ntrying URL 'https://raw.githubusercontent.com/OpenGene/fastp/master/testdata/R1.fq'\nContent type 'text/plain; charset=utf-8' length 3041 bytes\n==================================================\ndownloaded 3041 bytes\n\n‚Ä¢ built target read_1\n‚Ä¢ start target read_2\ntrying URL 'https://raw.githubusercontent.com/OpenGene/fastp/master/testdata/R2.fq'\nContent type 'text/plain; charset=utf-8' length 3343 bytes\n==================================================\ndownloaded 3343 bytes\n\n‚Ä¢ built target read_2\n‚Ä¢ start target fastp_out\n‚Ä¢ built target fastp_out\n‚Ä¢ end pipeline\n\nYou should be able to confirm that the read files were downloaded, cleaned, and a report generated in your working directory. Also, notice there is a new folder called _targets. This contains the metadata that targets uses to track each step of the pipeline (generally it should not be modified by hand; the same goes for the renv folder).\nNext steps\n\n\n\nFigure 3: Image by JOHN TOWNER on unsplash.\n\n\n\nThe example workflow just consists of a couple of steps, but I hope you can see how they are chained together: fastp_out depends on read_1 and read_2. We could add a third step that uses fastp_out for something else, and so forth.\nWe can also see this by visualizing the pipeline:\n\n\ntargets::tar_visnetwork()\n\n\n\n{\"x\":{\"nodes\":{\"name\":[\"fastp_out\",\"read_1\",\"read_2\"],\"type\":[\"stem\",\"stem\",\"stem\"],\"status\":[\"uptodate\",\"uptodate\",\"uptodate\"],\"seconds\":[1.381,0.411,0.415],\"bytes\":[420567,3041,3343],\"branches\":[null,null,null],\"id\":[\"fastp_out\",\"read_1\",\"read_2\"],\"label\":[\"fastp_out\",\"read_1\",\"read_2\"],\"level\":[2,1,1],\"color\":[\"#354823\",\"#354823\",\"#354823\"],\"shape\":[\"dot\",\"dot\",\"dot\"]},\"edges\":{\"from\":[\"read_1\",\"read_2\"],\"to\":[\"fastp_out\",\"fastp_out\"],\"arrows\":[\"to\",\"to\"]},\"nodesToDataframe\":true,\"edgesToDataframe\":true,\"options\":{\"width\":\"100%\",\"height\":\"100%\",\"nodes\":{\"shape\":\"dot\",\"physics\":false},\"manipulation\":{\"enabled\":false},\"edges\":{\"smooth\":{\"type\":\"cubicBezier\",\"forceDirection\":\"horizontal\"}},\"physics\":{\"stabilization\":false},\"layout\":{\"hierarchical\":{\"enabled\":true,\"direction\":\"LR\"}}},\"groups\":null,\"width\":null,\"height\":null,\"idselection\":{\"enabled\":false,\"style\":\"width: 150px; height: 26px\",\"useLabels\":true,\"main\":\"Select by id\"},\"byselection\":{\"enabled\":false,\"style\":\"width: 150px; height: 26px\",\"multiple\":false,\"hideColor\":\"rgba(200,200,200,0.5)\",\"highlight\":false},\"main\":{\"text\":\"\",\"style\":\"font-family:Georgia, Times New Roman, Times, serif;font-weight:bold;font-size:20px;text-align:center;\"},\"submain\":null,\"footer\":null,\"background\":\"rgba(0, 0, 0, 0)\",\"highlight\":{\"enabled\":true,\"hoverNearest\":false,\"degree\":{\"from\":1,\"to\":1},\"algorithm\":\"hierarchical\",\"hideColor\":\"rgba(200,200,200,0.5)\",\"labelOnly\":true},\"collapse\":{\"enabled\":true,\"fit\":false,\"resetHighlight\":true,\"clusterOptions\":null,\"keepCoord\":true,\"labelSuffix\":\"(cluster)\"},\"legend\":{\"width\":0.2,\"useGroups\":false,\"position\":\"right\",\"ncol\":1,\"stepX\":100,\"stepY\":100,\"zoom\":true,\"nodes\":{\"label\":[\"Up to date\",\"Stem\"],\"color\":[\"#354823\",\"#899DA4\"],\"shape\":[\"dot\",\"dot\"]},\"nodesToDataframe\":true}},\"evals\":[],\"jsHooks\":[]}\nTo keep things simple for this post, I have written the workflow as a single R script, but that‚Äôs not really the ideal way to do it. You can see that the syntax is rather verbose, and such a script would rapidly become very long. The best practice for targets workflows is to write the targets plan and the functions that build each target separately, as _targets.R and functions.R, respectively.\nBy splitting the plan from the functions this way, our _targets.R file becomes much shorter and more readable:\n\n\nlibrary(targets)\nlibrary(tarchetypes)\nlibrary(babelwhale)\n\n# Set babelwhale backend for running containers\nset_default_config(create_docker_config())\n\n# Load functions\nsource(\"R/functions.R\")\n\ntar_plan(\n  # Download example fastq files\n  tar_file(read_1, download_read(\"R1.fq\")),\n  tar_file(read_2, download_read(\"R2.fq\")),\n  # Clean the fastq files with fastp\n  tar_file(\n    fastp_out, \n    fastp(read_1, read_2, \"R1_trim.fq\", \"R2_trim.fq\", \"trim_report.html\"\n    )\n  )\n)\n\n\n\nYou can see how it provides a high-level overview of each step in the workflow, without getting bogged down in the details. And the best part is, you don‚Äôt have to install fastp (or any other software used for a particular step)! Docker takes care of that for you.\nFurthermore, thanks to targets, if one part of the workflow changes and we run tar_make() again, only the part that changed will be run. Try it by deleting R1.fq, then run tar_make() again and see what happens.\nI have made this plan and the accompanying functions.R file available at this repo: https://github.com/joelnitta/targets_bioinfo_example. Please check it out!\nConclusion\nI am really excited about using targets for reproducibly managing bioinformatics workflows from R. I hope this helps others who may want to do the same!\n\n\nLast updated\n2021-11-22 12:35:03 JST\nDetails\nsource code, R environment\n\n\n\n\n\n\nDocker images are basically completely self-contained computing environments, such that the software inside the image is exactly the same no matter where it is run. A major benefit of using a docker image is that you don‚Äôt have to install all of the various dependencies for a particular package: it all comes bundled in the image. And if the image has been tagged (versioned) correctly, you can specify the exact software version and know that the results won‚Äôt change in the future.‚Ü©Ô∏é\nBioconda creates a Docker image for each package, which is listed in Biocontainers and uploaded to Quay.io, so Bioconda and Biocontainers are largely overlapping. I find the Bioconda interface easier to use for finding images. You can also just try googling the name of the software you want to use plus ‚Äúdocker‚Äù. If there is no available image, you can build one yourself, but that‚Äôs outside the scope of this post.‚Ü©Ô∏é\nThink Inception.‚Ü©Ô∏é\nConda was originally developed for managing python and python packages, but it has expanded greatly and works as a general software package manager.‚Ü©Ô∏é\nThe asterisk in r-renv=0.14.* indicates to install the most recent version with the 0.14 version number.‚Ü©Ô∏é\nI‚Äôm not actually running this command and showing the output, since this post is already rendered using renv, and running renv within renv is also getting too Inception-y!‚Ü©Ô∏é\nI typically use Docker, but Singularity may be a good option if you want to run your workflow on a machine where you don‚Äôt have root privileges (such as on a cluster). Docker requires root privileges to install, but Singularity doesn‚Äôt (for that matter neither does Conda). I have not tested any of this with Singularity.‚Ü©Ô∏é\nI won‚Äôt go into the details of the targets syntax here, but I highly recommend this chapter in the targets manual for working with external files, which are very common in bioinformatics workflows.‚Ü©Ô∏é\n",
    "preview": "posts/2021-11-16_r-bioinfo-flow/img/t-k-9AxFJaNySB8-unsplash.jpg",
    "last_modified": "2022-03-07T08:13:41+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-02_color-scheme-anc-states/",
    "title": "Selecting color schemes for mapping ancestral states",
    "description": "How to change the phytools default color scheme when visualizing the results of ancestral character state estimation",
    "author": [
      {
        "name": "Joel Nitta",
        "url": "https://joelnitta.com"
      }
    ],
    "date": "2021-06-02",
    "categories": [],
    "contents": "\n\n\nThe phytools package provides (among many other things) the contMap() function for estimating ancestral character states and visualizing their changes along the branches of a phylogenetic tree. It can either produce the plot directly (default), or be saved as an object with the plot = FALSE argument, to be further manipulated and plotted later with plot().\nDefault colors\nI have to say I‚Äôm not a fan of the default color scheme, which is a rainbow palette going from red through yellow and green to blue.\nFor example, let‚Äôs borrow some example code and look at the default plot:\n\n\n# code modified slightly from http://www.phytools.org/eqg2015/asr.html\n\n## Load needed packages for this blogpost\nlibrary(phytools)\nlibrary(ggtree)\nlibrary(tidyverse)\nlibrary(scico)\nlibrary(viridisLite)\n\n## Load anole tree\nanole.tree <- read.tree(\"http://www.phytools.org/eqg2015/data/anole.tre\")\n\n## Load anole trait data, extract snout-vent-length (svl) as named vector\nsvl <- read_csv(\"http://www.phytools.org/eqg2015/data/svl.csv\") %>%\n  mutate(svl = set_names(svl, species)) %>%\n  pull(svl)\n\n# Plot with default color scheme\ncontmap_obj <- contMap(anole.tree, svl, plot = FALSE)\n\nplot(\n  contmap_obj, \n  type=\"fan\", \n  legend = 0.7*max(nodeHeights(anole.tree)),\n  fsize = c(0.5, 0.7))\n\n\n\n\nAlthough this does provide a wide range of colors, it‚Äôs not obvious why one color is greater or less than the others. In particular it‚Äôs hard to discern the order of intermediate values (yellow, green, light blue). Indeed, there has been much written on why the rainbow palette is generally not a good way to visualize continuous data.\nDefining a new color palette\nphytools::setMap() can be used to specify another color palette. setMap() passes its second argument (a vector of color names or hexadecimals) to colorRampPalette(). colorRampPalette() is a bit unusual in that it‚Äôs a function that produces a function, in this case, one that generates a vector of colors interpolating between the original input values:\n\n\n# colorRampPalette() produces a function\nmy_color_func <- colorRampPalette(c(\"red\", \"yellow\"))\nclass(my_color_func)\n\n\n[1] \"function\"\n\n# The function generates n colors interpolating between\n# the colors originally passed to colorRampPalette()\nmy_colors <- my_color_func(n = 6)\nscales::show_col(my_colors)\n\n\n\n\nSo, this works fine for generating custom color gradients. But designing accurate, color-blind friendly color palettes is not a simple task. Fortunately, there are several packages available with such carefully crafted palettes. Two of my favorite are viridis and scico. How can we use these with the plotting function in phytools?\nUsing viridis or scico palettes\nWell, it turns out that as long as we specify the same number of colors, we can replicate the viridis color palette with colorRampPalette(). The only difference is the alpha, or transparency level, indicated at the end of each hexidecimal with two letters (here ‚ÄúFF‚Äù). There is no reason to use transparency here anyways, so that doesn‚Äôt matter.\n\n\n# viridis color palette with 6 colors\nviridis(6)\n\n\n[1] \"#440154FF\" \"#414487FF\" \"#2A788EFF\" \"#22A884FF\" \"#7AD151FF\"\n[6] \"#FDE725FF\"\n\n# colorRampPalette() replicating viridis color palette\ncolorRampPalette(viridis(6))(6)\n\n\n[1] \"#440154\" \"#414487\" \"#2A788E\" \"#22A884\" \"#7AD151\" \"#FDE725\"\n\nSo here is the viridis version of the phytools plot:\n\n\n# Count the number of unique character states in the observed data:\nn_cols <- n_distinct(svl)\n\n# Change the color palette\ncontmap_obj_viridis <- setMap(contmap_obj, viridis(n_cols))\n\n# Plot the mapped characters with the new colors\nplot(\n  contmap_obj_viridis, \n  type=\"fan\", \n  legend = 0.7*max(nodeHeights(anole.tree)),\n  fsize = c(0.5, 0.7))\n\n\n\n\nAnd here is another one, this time using a palette from scico:\n\n\n# Change the color palette\ncontmap_obj_scico <- setMap(contmap_obj, scico(n_cols, palette = \"bilbao\"))\n\n# Plot the mapped characters with the new colors\nplot(\n  contmap_obj_scico, \n  type=\"fan\", \n  legend = 0.7*max(nodeHeights(anole.tree)),\n  fsize = c(0.5, 0.7))\n\n\n\n\nI personally find this one even easier to interpret than viridis. It‚Äôs very clear which values are low and high.\nggtree\nJust for completeness, here is code to replicate the plot in ggtree.\n\n\n# Modified from https://yulab-smu.top/treedata-book/chapter4.html#color-tree\n\n# Fit an ancestral state character reconstruction\nfit <- phytools::fastAnc(anole.tree, svl, vars = TRUE, CI = TRUE)\n\n# Make a dataframe with trait values at the tips\ntd <- data.frame(\n  node = nodeid(anole.tree, names(svl)),\n  trait = svl)\n\n# Make a dataframe with estimated trait values at the nodes\nnd <- data.frame(node = names(fit$ace), trait = fit$ace)\n\n# Combine these with the tree data for plotting with ggtree\nd <- rbind(td, nd)\nd$node <- as.numeric(d$node)\ntree <- full_join(anole.tree, d, by = 'node')\n\nggtree(\n  tree, \n  aes(color = trait), \n  layout = 'circular', \n  ladderize = FALSE, continuous = \"color\", size = 1) +\n  # >>> The important part! <<<\n  # Choose your favorite scale_color_* function here: \n  scale_color_scico(palette = \"bilbao\") + \n  geom_tiplab(hjust = -.1, size = 2, color = \"black\") + \n  xlim(0, 1.2) + \n  theme(\n    legend.position = c(0, .82),\n    legend.text = element_text(size = 8),\n    legend.title = element_text(size = 8)\n  ) \n\n\n\n\nThat‚Äôs it!\n\n\nLast updated\n2021-11-22 14:42:13 JST\nDetails\nsource code, R environment\n\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-06-02_color-scheme-anc-states/featured.png",
    "last_modified": "2022-03-07T08:13:40+00:00",
    "input_file": {},
    "preview_width": 1024,
    "preview_height": 920
  },
  {
    "path": "posts/2019-02-16_building-r-docker-images-with-secrets/",
    "title": "Building R docker images with secrets",
    "description": "Keep it secret. Keep it safe.",
    "author": [
      {
        "name": "Joel Nitta",
        "url": "https://joelnitta.com"
      }
    ],
    "date": "2019-02-16",
    "categories": [],
    "contents": "\n\n\n\n\n\n\nFigure 1: Image in the public domain on wikimedia.\n\n\n\nDocker is an incredibly useful tool for running reproducible analysis workflows. For useRs, the rocker collection of images is very convenient for creating version-controlled R environments. This is pretty straightforward if you are using packages on CRAN, or publicly available packages on GitHub. But what if we want to use private packages on GitHub, or need for any other reason to enter authentication credentials during the build?\nThere are various ways to copy data into the image during the build, but when handling secrets that we don‚Äôt want hanging around after it‚Äôs finished, caution is needed. Approaches such as using COPY or ARGS will leave traces in the build. Staged builds are more secure, but tricky. Fortunately, as of v. 18.09, Docker is now providing official support for handling secrets.\nA simple example\nHere is how to use the new Docker features to securely pass a secret during a build 1.\nThere are few non-default settings that need to be specified for this. First of all, prior to the docker build command, you need to specify that you want to use the new BuildKit backend with DOCKER_BUILDKIT=1. So the command starts DOCKER_BUILDKIT=1 docker build ...\nNext, we must add a syntax directive to the top line of the Dockerfile. For example, for a Dockerfile based on rocker/tidyverse:\n# syntax=docker/dockerfile:1.0.0-experimental\nFROM rocker/tidyverse\nSave your secrets in a text file. Let‚Äôs call it my_secret_stash2. If you are using it to store your GitHub PAT, it would just be one line with the PAT. Here, let‚Äôs put in some random word:\necho \"FABULOUS\" > my_secret_stash\nThis is all we need to use secrets during the build. Here is an example Dockerfile similar to the one in the Docker documentation.\n# syntax = docker/dockerfile:1.0-experimental\nFROM alpine\n\nRUN --mount=type=secret,id=mysecret cat /run/secrets/mysecret\nTo see how it works, save this as Dockerfile, then from the same directory containing Dockerfile and my_secret_stash, build the image:\nDOCKER_BUILDKIT=1 docker build --progress=plain --no-cache \\\n--secret id=mysecret,src=my_secret_stash .\nI‚Äôve truncated the output, but you should see something like this (the exact build step number may vary).\n\n#7 [2/2] RUN --mount=type=secret,id=mysecret cat /run/secrets/mysecret\n#7       digest: sha256:75601a522ebe80ada66dedd9dd86772ca932d30d7e1b11bba94c04aa55c237de\n#7         name: \"[2/2] RUN --mount=type=secret,id=mysecret cat /run/secrets/mysecret\"\n#7      started: 2019-02-18 20:51:20.1092144 +0000 UTC\n#7 0.668 FABULOUS\n#7    completed: 2019-02-18 20:51:21.0927656 +0000 UTC\n#7     duration: 983.5512ms\n\nCan you spot our secret? It‚Äôs showing up from the cat command. However, it will not remain in the image.\nInstalling a private R package\nTo install a package from my private GitHub repo, I created an additional simple R script, called install_git_packages.R:\nsecret <- commandArgs(trailing = TRUE)\ndevtools::install_github(\"joelnitta/my-private-package\", auth_token = secret)\ncommandArgs(trailing = TRUE) will return whatever command line arguments were passed to Rscript after the name of the script, as a character vector.\nWe will call this script from the Dockerfile and pass the secret to it.\nHere is the Dockerfile to do that. (Note that although we copy the install_git_packages.R script into the image, we are passing it the secret variable that is only present during the build, so this should not remain afterwards.)\n# syntax = docker/dockerfile:1.0-experimental\nFROM rocker/tidyverse:3.5.1\n\nENV DEBIAN_FRONTEND noninteractive\n\nCOPY install_git_packages.R .\n\nRUN apt-get update\n\nRUN --mount=type=secret,id=mysecret \\\nRscript install_git_packages.R `cat /run/secrets/mysecret`\nLet‚Äôs build the image and tag it:\nDOCKER_BUILDKIT=1 docker build --progress=plain --no-cache \\\n--secret id=mysecret,src=my_secret_stash . -t my_special_image\nThat‚Äôs it!\n\n\nLast updated\n2021-11-22 14:43:03 JST\nDetails\nsource code, R environment\n\n\n\n\n\n\nNo guarantees!! This is just my understanding from reading the docker documentation and other blogs‚Ü©Ô∏é\nOf course, be sure to add the file containing the secret to .gitignore!‚Ü©Ô∏é\n",
    "preview": "posts/2019-02-16_building-r-docker-images-with-secrets/featured.png",
    "last_modified": "2022-03-07T08:13:40+00:00",
    "input_file": {},
    "preview_width": 300,
    "preview_height": 150
  }
]
